{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6658d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899a909",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import pathlib\n",
    "from typing import Optional, Dict, List\n",
    "import logging\n",
    "import nvidia_smi\n",
    "import os\n",
    "import s3fs\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AutoPeftModelForCausalLM\n",
    ")\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\",\n",
    "        metadata={\"help\": \"The model that you want to train from Huggingface. Defaults to Meta's Llama2 7B-chat and requires a HF login\"}\n",
    "    )\n",
    "    new_model_name: Optional[str] = field(\n",
    "        default=\"agora-llama-7b-chat\",\n",
    "        metadata={\"help\": \"The name for your fine-tuned model\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    hf_data_path: str = field(\n",
    "        default=\"iamtarun/python_code_instructions_18k_alpaca\",\n",
    "        metadata={\"help\": \"The path to the HF dataset. Defaults to `iamtarun/python_code_instructions_18k_alpaca`\"}\n",
    "    )\n",
    "    split: Optional[str] = field(\n",
    "        default=\"train\", #TODO: should this be default?,\n",
    "        metadata={\"help\": \"Which portion of the dataset you want to use\"}\n",
    "    )\n",
    "    personal_data: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path to your proprietary data\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingArguments(TrainingArguments):\n",
    "    # Specify an additional cache dir for files downloaded during training\n",
    "    # Usually things are downloaded into ~/.cache/huggingface\n",
    "    # Adding this is helpful for distributed training where all workers should read from a central cache \n",
    "    cache_dir : Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    )\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Different models have different max lengths but this keeps it at a standard 512 incase you don't specify. Seq might be truncated\"}\n",
    "    )\n",
    "    output_dir : str = field(\n",
    "        default=\"results\", \n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    ) \n",
    "    num_train_epochs : int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of training epochs\"}\n",
    "    )\n",
    "    fp16 : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enable fp16 training\"}\n",
    "    )\n",
    "    bf16 : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable bf16 training. Only possible on A100 GPUs\"}\n",
    "    )\n",
    "    per_device_train_batch_size : bool = field(\n",
    "        default=10,\n",
    "        metadata={\"help\": \"Training batch size per device\"}\n",
    "    )\n",
    "    gradient_accumulation_steps : int = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\"}\n",
    "    )\n",
    "    gradient_checkpointing : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \" If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"}\n",
    "    )\n",
    "    max_grad_norm : float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Maximum gradient normal (gradient clipping)\"}\n",
    "    )\n",
    "    learning_rate : float = field(\n",
    "        default=2e-4,\n",
    "        metadata={\"help\": \"Initial learning rate (AdamW optimizer)\"}\n",
    "    )\n",
    "    weight_decay : float = field(\n",
    "        default=0.001,\n",
    "        metadata={\"help\": \"Weight decay to apply to all layers except bias/LayerNorm weights\"}\n",
    "    )\n",
    "    optim : str = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"Optimizer to use for training\"}\n",
    "    )\n",
    "    lr_scheduler_type : str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule (constant a bit better than cosine)\"}\n",
    "    )\n",
    "    warmup_ratio : float = field(\n",
    "        default=0.03,\n",
    "        metadata={\"help\": \"Ratio of steps for a linear warmup (from 0 to learning rate)\"}\n",
    "    )\n",
    "    group_by_length : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Group sequences into batches with same length - Saves memory and speeds up training considerably\"}\n",
    "    )\n",
    "    save_steps : int = field(\n",
    "        default=5,\n",
    "        metadata={\"help\": \"Save checkpoint every X updates steps\"}\n",
    "    )\n",
    "    save_total_limit: int = field(\n",
    "        default=2,\n",
    "        metadata={}\n",
    "    )\n",
    "    logging_steps : int = field(\n",
    "        default=25,\n",
    "        metadata={\"help\": \"Log every X updates steps\"}\n",
    "    )\n",
    "    max_seq_length : int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\":\"Maximum sequence length to use\"}\n",
    "    )\n",
    "    packing : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"Pack multiple short examples in the same input sequence to increase efficiency\"}\n",
    "    )\n",
    "    device_map : any = field(\n",
    "        default_factory=(lambda: {\"\":0}),\n",
    "        metadata={\"help\":\"Device mapping for the SFTTrainer\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuanitzationArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    load_in_4bit: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Load a model in 4bit\"}\n",
    "    )\n",
    "    bnb_4bit_compute_dtype: torch.dtype = field(\n",
    "        default=torch.float16, \n",
    "        metadata={\"help\": \"Compute dtype for 4-bit base models\"}\n",
    "    )\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\", \n",
    "        metadata={\"help\": \"Quantization type (fp4 or nf4)\"}\n",
    "    )\n",
    "    use_nested_quant: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate nested quantization for 4-bit base models (double quantization)\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class QloraArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=64, \n",
    "        metadata={\"help\": \"LoRA attention dimension\"}\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=16, \n",
    "        metadata={\"help\": \"Alpha parameter for LoRA scaling\"}\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.1, \n",
    "        metadata={\"help\": \"Dropout probability for LoRA layers\"}\n",
    "    )\n",
    "    bias: Optional[str] = field(\n",
    "        default=\"none\",\n",
    "        metadata={}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"CAUSAL_LM\",\n",
    "        metadata={}\n",
    "    )\n",
    "\n",
    "class CheckpointCallback(TrainerCallback):\n",
    "   def __init__(self, training_args) -> None:\n",
    "       self.training_args = training_args\n",
    "   \n",
    "   def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        MY_ENCRYPTION_PASSPHRASE = \"snake enrich route baby ribbon space share corn believe destroy have turtle\"\n",
    "        ACCESS_KEY_ID = \"jut557u3lugpwz6aelkwx6ajsdiq\"\n",
    "        SECRET_ACCESS_KEY = \"j2hinpshm7fyz4dkn64aq7dg2dmw766ms3tb3i6vl7mvkktofhucg\"\n",
    "        ENDPOINT_URL = \"https://gateway.storjshare.io/\"\n",
    "\n",
    "        storage_options = {\n",
    "            \"key\": ACCESS_KEY_ID,\n",
    "            \"secret\": SECRET_ACCESS_KEY,\n",
    "            \"client_kwargs\": {\n",
    "                \"endpoint_url\": ENDPOINT_URL\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        s3 = s3fs.S3FileSystem(**storage_options)\n",
    "       \n",
    "        ckpt_dir = self.training_args.output_dir\n",
    "        ckpt_name = f\"checkpoint-{state.global_step}\"\n",
    "        ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n",
    "        for filename in os.listdir(ckpt_path):\n",
    "            s3_key = f\"{ckpt_name}/{filename}\"\n",
    "            with s3.open(f's3://demo-bucket/{s3_key}', 'wb') as f:\n",
    "                f.write(open(os.path.join(ckpt_path, filename), 'rb').read())\n",
    "\n",
    "        \n",
    "def huggingface_login():\n",
    "    #try: \n",
    "    #    HUGGING_FACE_TOKEN = os.environ['HUGGING_FACE_TOKEN']\n",
    "    #except KeyError:\n",
    "    #    raise Exception('Need to pass hugging face access token as environment variable.')\n",
    "\n",
    "    login(token=\"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\")\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    # Get model state dict containing weights at time of call\n",
    "    # Convert to CPU tensors -> reduced memory?\n",
    "    # Delete original state dict to free VRAM\n",
    "    # _save() call to save it to disk/or external storage...?\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save():\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)\n",
    "\n",
    "def preprocess_data(source, tokenizer: PreTrainedTokenizer) -> Dict:\n",
    "    return {}\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "\n",
    "def build_bnb_config(quant_args) -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=quant_args.load_in_4bit,\n",
    "        bnb_4bit_quant_type=quant_args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=quant_args.bnb_4bit_compute_dtype\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def build_lora_config(qlora_args) -> LoraConfig:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=qlora_args.lora_alpha,\n",
    "        lora_dropout=qlora_args.lora_dropout,\n",
    "        r=qlora_args.lora_r,\n",
    "        bias=qlora_args.bias,\n",
    "        task_type=qlora_args.task_type\n",
    "    )\n",
    "    return peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fe3de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def finetune():\n",
    "    huggingface_login()\n",
    "\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, ModelTrainingArguments, QuanitzationArguments, QloraArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args, quant_args, qlora_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    # logic to restart from checkpoint\n",
    "    resume_from_checkpoint = False\n",
    "    checkpoints = list(\n",
    "        pathlib.Path(training_args.output_dir).glob('checkpoint-*'))\n",
    "    if checkpoints:\n",
    "        resume_from_checkpoint = True\n",
    "\n",
    "    bnb_config = build_bnb_config(quant_args=quant_args)\n",
    "    peft_config = build_lora_config(qlora_args=qlora_args)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset = load_dataset(data_args.hf_data_path, split=data_args.split)\n",
    "    dataset = dataset.remove_columns(['instruction', 'input', 'output']) #TODO: this is python dataset specific preprocessing. Will need to handle this inside preprocess function somehow\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  \n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\", #TODO: this will change based on dataset. I would add this as an optional default into DataArguments\n",
    "        max_seq_length=None,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(CheckpointCallback(training_args=training_args)) #could just pass in output_dir here\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    trainer.save_state() #grabbed from skypilot but need to understand state better\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2908e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638259d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
