{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966f975-8ee5-41b0-b1f9-6d43515d1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3a09e-e5f0-4e27-82c2-c2e948155d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a899a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "import logging\n",
    "import nvidia_smi\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AutoPeftModelForCausalLM\n",
    ")\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from helpers.storj import Storj\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\",\n",
    "        metadata={\"help\": \"The model that you want to train from Huggingface. Defaults to Meta's Llama2 7B-chat and requires a HF login\"}\n",
    "    )\n",
    "    new_model_name: Optional[str] = field(\n",
    "        default=\"agora-llama-7b-chat\",\n",
    "        metadata={\"help\": \"The name for your fine-tuned model\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    hf_data_path: str = field(\n",
    "        default=\"iamtarun/python_code_instructions_18k_alpaca\",\n",
    "        metadata={\"help\": \"The path to the HF dataset. Defaults to `iamtarun/python_code_instructions_18k_alpaca`\"}\n",
    "    )\n",
    "    split: Optional[str] = field(\n",
    "        default=\"train\", #TODO: should this be default?,\n",
    "        metadata={\"help\": \"Which portion of the dataset you want to use\"}\n",
    "    )\n",
    "    personal_data: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path to your proprietary data\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingArguments(TrainingArguments):\n",
    "    # Specify an additional cache dir for files downloaded during training\n",
    "    # Usually things are downloaded into ~/.cache/huggingface\n",
    "    # Adding this is helpful for distributed training where all workers should read from a central cache\n",
    "    job_id : int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Unique id for the model training job\"}\n",
    "    )\n",
    "    bucket_name : Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the Storj bucket to upload/download checkpoints to and from\"}\n",
    "    )\n",
    "    cache_dir : Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    )\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Different models have different max lengths but this keeps it at a standard 512 incase you don't specify. Seq might be truncated\"}\n",
    "    )\n",
    "    output_dir : str = field(\n",
    "        default=\"./results\",\n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    ) \n",
    "    # num_train_epochs : int = field(\n",
    "    #     default=1,\n",
    "    #     metadata={\"help\": \"Number of training epochs\"}\n",
    "    # )\n",
    "    fp16 : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enable fp16 training\"}\n",
    "    )\n",
    "    bf16 : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable bf16 training. Only possible on A100 GPUs\"}\n",
    "    )\n",
    "    per_device_train_batch_size : int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Training batch size per device\"}\n",
    "    )\n",
    "    gradient_accumulation_steps : int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\"}\n",
    "    )\n",
    "    gradient_checkpointing : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \" If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"}\n",
    "    )\n",
    "    max_grad_norm : float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Maximum gradient normal (gradient clipping)\"}\n",
    "    )\n",
    "    learning_rate : float = field(\n",
    "        default=2e-4,\n",
    "        metadata={\"help\": \"Initial learning rate (AdamW optimizer)\"}\n",
    "    )\n",
    "    weight_decay : float = field(\n",
    "        default=0.001,\n",
    "        metadata={\"help\": \"Weight decay to apply to all layers except bias/LayerNorm weights\"}\n",
    "    )\n",
    "    optim : str = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"Optimizer to use for training\"}\n",
    "    )\n",
    "    lr_scheduler_type : str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule (constant a bit better than cosine)\"}\n",
    "    )\n",
    "    warmup_ratio : float = field(\n",
    "        default=0.03,\n",
    "        metadata={\"help\": \"Ratio of steps for a linear warmup (from 0 to learning rate)\"}\n",
    "    )\n",
    "    group_by_length : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Group sequences into batches with same length - Saves memory and speeds up training considerably\"}\n",
    "    )\n",
    "    save_steps : int = field(\n",
    "        default=25,\n",
    "        metadata={\"help\": \"Save checkpoint every X updates steps\"}\n",
    "    )\n",
    "    save_total_limit: int = field(\n",
    "        default=2,\n",
    "        metadata={}\n",
    "    )\n",
    "    logging_steps : int = field(\n",
    "        default=25,\n",
    "        metadata={\"help\": \"Log every X updates steps\"}\n",
    "    )\n",
    "    max_seq_length : int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\":\"Maximum sequence length to use\"}\n",
    "    )\n",
    "    packing : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"Pack multiple short examples in the same input sequence to increase efficiency\"}\n",
    "    )\n",
    "    device_map : any = field(\n",
    "        default_factory=(lambda: {\"\":0}),\n",
    "        metadata={\"help\":\"Device mapping for the SFTTrainer\"}\n",
    "    )\n",
    "    max_steps: int = field(\n",
    "        default=20,\n",
    "        metadata={}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class QuanitzationArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    load_in_4bit: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Load a model in 4bit\"}\n",
    "    )\n",
    "    bnb_4bit_compute_dtype: torch.dtype = field(\n",
    "        default=torch.float16, \n",
    "        metadata={\"help\": \"Compute dtype for 4-bit base models\"}\n",
    "    )\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\", \n",
    "        metadata={\"help\": \"Quantization type (fp4 or nf4)\"}\n",
    "    )\n",
    "    use_nested_quant: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate nested quantization for 4-bit base models (double quantization)\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class QloraArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=64, \n",
    "        metadata={\"help\": \"LoRA attention dimension\"}\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=16, \n",
    "        metadata={\"help\": \"Alpha parameter for LoRA scaling\"}\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.1, \n",
    "        metadata={\"help\": \"Dropout probability for LoRA layers\"}\n",
    "    )\n",
    "    bias: Optional[str] = field(\n",
    "        default=\"none\",\n",
    "        metadata={}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"CAUSAL_LM\",\n",
    "        metadata={}\n",
    "    )\n",
    "\n",
    "class CheckpointCallback(TrainerCallback):\n",
    "    def __init__(self, training_args: TrainingArguments, storj: Storj) -> None:\n",
    "        self.training_args = training_args\n",
    "        self.storj = storj\n",
    "   \n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        self.storj.save_checkpoints_to_cloud(args.output_dir, state.global_step, args.job_id)\n",
    "        \n",
    "def huggingface_login():\n",
    "    # try: \n",
    "    #     HUGGING_FACE_TOKEN = os.environ['HUGGING_FACE_TOKEN']\n",
    "    # except KeyError:\n",
    "    #     raise Exception('Need to pass hugging face access token as environment variable.')\n",
    "\n",
    "    login(token=\"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\")\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    \"\"\"\n",
    "    - Get model state dict containing weights at time of call\n",
    "    - Convert to CPU tensors -> reduced memory?\n",
    "    - Delete original state dict to free VRAM\n",
    "    - _save() call to save it to disk/or external storage...?\n",
    "    \"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)\n",
    "\n",
    "def preprocess_data(source, tokenizer: PreTrainedTokenizer) -> Dict:\n",
    "    return {}\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "\n",
    "def build_bnb_config(quant_args) -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=quant_args.load_in_4bit,\n",
    "        bnb_4bit_quant_type=quant_args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=quant_args.bnb_4bit_compute_dtype\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def build_lora_config(qlora_args) -> LoraConfig:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=qlora_args.lora_alpha,\n",
    "        lora_dropout=qlora_args.lora_dropout,\n",
    "        r=qlora_args.lora_r,\n",
    "        bias=qlora_args.bias,\n",
    "        task_type=qlora_args.task_type\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "def finetune():\n",
    "    huggingface_login()\n",
    "\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, ModelTrainingArguments, QuanitzationArguments, QloraArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args, quant_args, qlora_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True) #TODO: remaining and the argument were added due to a weird error on Vast\n",
    "\n",
    "    # if bucket_name is not '', check for checkpoints in user's bucket\n",
    "    resume_from_checkpoint = False\n",
    "    if training_args.bucket_name:\n",
    "        storj = Storj(training_args.bucket_name)\n",
    "        resume_from_checkpoint = storj.pull_checkpoints_from_cloud(training_args)\n",
    "\n",
    "    bnb_config = build_bnb_config(quant_args=quant_args)\n",
    "    peft_config = build_lora_config(qlora_args=qlora_args)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset = load_dataset(data_args.hf_data_path, split=data_args.split)\n",
    "    dataset = dataset.remove_columns(['instruction', 'input', 'output']) #TODO: this is python dataset specific preprocessing. Will need to handle this inside preprocess function somehow\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  \n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\", #TODO: this will change based on dataset. I would add this as an optional default into DataArguments\n",
    "        max_seq_length=None,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "    if training_args.bucket_name:\n",
    "        trainer.add_callback(CheckpointCallback(training_args, storj))\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    trainer.save_state() #grabbed from skypilot but need to understand state better\n",
    "    print(\"state saved\")\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=training_args.output_dir)\n",
    "    print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d79e41-69f3-4027-a634-1590b7a347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fba5df-6904-4ba1-9594-807b35418a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bnb_config() -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "bnb = build_bnb_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a973da-6c75-42cb-b77e-4087206a8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\"results\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98604f5a-269f-4acd-85ae-922b8a34a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d60bc-4b72-49d4-9c10-a54fee211f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598baba3-e2e4-4ea8-bd30-fdbc75b04248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df640215-6442-4727-b6b2-7a748887f8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be0db078504ab7868b2fa61bb9afba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"results/llama2/final_merged_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b72802-4b32-4c22-89d4-7eaad4caea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da44f6f2-d89b-4182-a7f6-a1c7c1fc6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3161d5d8-0870-49f9-9641-2332e0cdcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1475/3577709305.py:1: GradioDeprecationWarning: `server_name` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
      "  demo = gr.Interface.from_pipeline(llama_pipeline, server_name=\"0.0.0.0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://f6e212afee100e178c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f6e212afee100e178c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/routes.py\", line 507, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 641, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/pipelines.py\", line 222, in fn\n",
      "    data = pipeline(**data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 204, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1129, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1136, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1035, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 265, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1402, in generate\n",
      "    self._validate_model_class()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1197, in _validate_model_class\n",
      "    raise TypeError(exception_message)\n",
      "TypeError: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/routes.py\", line 507, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 641, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/pipelines.py\", line 222, in fn\n",
      "    data = pipeline(**data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 204, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1129, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1136, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1035, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 265, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1402, in generate\n",
      "    self._validate_model_class()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1197, in _validate_model_class\n",
      "    raise TypeError(exception_message)\n",
      "TypeError: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface.from_pipeline(llama_pipeline)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6a9cf-3e41-4344-aed2-45eafbf3ac4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
