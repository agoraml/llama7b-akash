{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe6658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools_rust==1.7.0\n",
      "  Downloading setuptools_rust-1.7.0-py3-none-any.whl (25 kB)\n",
      "Collecting setuptools==68.1.2\n",
      "  Downloading setuptools-68.1.2-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.1/805.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wheel==0.41.2\n",
      "  Downloading wheel-0.41.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.5.0\n",
      "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.22.0\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.14.4\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.41.1\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-ml-py3==7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy==1.11.2\n",
      "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.7.1\n",
      "  Downloading trl-0.7.1-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.0.1)\n",
      "Collecting transformers==4.32.1\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting s3fs==2023.9.0\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl (28 kB)\n",
      "Collecting semantic-version<3,>=2.8.2\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tomli>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from setuptools_rust==1.7.0->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from setuptools_rust==1.7.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (23.0)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (1.24.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (6.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4->-r requirements.txt (line 6)) (2.29.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 11)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 11)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 11)) (3.1.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting aiobotocore~=2.5.4\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.31.18,>=1.31.17\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 6)) (2.0.4)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 6)) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4->-r requirements.txt (line 6)) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4->-r requirements.txt (line 6)) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4->-r requirements.txt (line 6)) (2022.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 11)) (1.3.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.4->-r requirements.txt (line 6)) (1.16.0)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=ceb0d98ffce8f646964dd71eb7aa0dc3bfbf6cd0b4e2223fa4dc89adf4ed7852\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: tokenizers, safetensors, nvidia-ml-py3, bitsandbytes, xxhash, wrapt, wheel, tzdata, setuptools, semantic-version, scipy, regex, pyarrow, multidict, jmespath, fsspec, frozenlist, dill, async-timeout, aioitertools, yarl, setuptools_rust, pandas, multiprocess, huggingface-hub, botocore, aiosignal, transformers, aiohttp, accelerate, peft, aiobotocore, s3fs, datasets, trl\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.6.3\n",
      "    Uninstalling setuptools-65.6.3:\n",
      "      Successfully uninstalled setuptools-65.6.3\n",
      "Successfully installed accelerate-0.22.0 aiobotocore-2.5.4 aiohttp-3.8.5 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.41.1 botocore-1.31.17 datasets-2.14.4 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.9.0 huggingface-hub-0.17.1 jmespath-1.0.1 multidict-6.0.4 multiprocess-0.70.15 nvidia-ml-py3-7.352.0 pandas-2.1.0 peft-0.5.0 pyarrow-13.0.0 regex-2023.8.8 s3fs-2023.9.0 safetensors-0.3.3 scipy-1.11.2 semantic-version-2.10.0 setuptools-68.1.2 setuptools_rust-1.7.0 tokenizers-0.13.3 transformers-4.32.1 trl-0.7.1 tzdata-2023.3 wheel-0.41.2 wrapt-1.15.0 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6966f975-8ee5-41b0-b1f9-6d43515d1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-3.44.0-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gradio-client==0.5.0\n",
      "  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Collecting matplotlib~=3.0\n",
      "  Downloading matplotlib-3.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources<7.0,>=1.3\n",
      "  Downloading importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting altair<6.0,>=4.2.0\n",
      "  Downloading altair-5.1.1-py3-none-any.whl (520 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.6/520.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.5.0)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.24.3)\n",
      "Collecting websockets<12.0,>=10.0\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (23.0)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.4.0)\n",
      "Collecting orjson~=3.0\n",
      "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.0)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.17.1)\n",
      "Requirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.29.0)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.5.0->gradio) (2023.9.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions~=4.0\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting pydantic-core==2.6.3\n",
      "  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (1.26.15)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<4.0.0,>=3.7.1\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpcore<0.19.0,>=0.18.0\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7b0b7965db1dd3a733d89f12f2dcb5931264f75fbdfb353644132147b2e5b8d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, python-multipart, pyparsing, orjson, kiwisolver, importlib-resources, h11, fonttools, cycler, contourpy, click, anyio, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, matplotlib, httpcore, pydantic, httpx, altair, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.0\n",
      "    Uninstalling anyio-3.7.0:\n",
      "      Successfully uninstalled anyio-3.7.0\n",
      "Successfully installed aiofiles-23.2.1 altair-5.1.1 annotated-types-0.5.0 anyio-3.7.1 click-8.1.7 contourpy-1.1.0 cycler-0.11.0 fastapi-0.103.1 ffmpy-0.3.1 fonttools-4.42.1 gradio-3.44.0 gradio-client-0.5.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 importlib-resources-6.0.1 kiwisolver-1.4.5 matplotlib-3.7.3 orjson-3.9.7 pydantic-2.3.0 pydantic-core-2.6.3 pydub-0.25.1 pyparsing-3.1.1 python-multipart-0.0.6 starlette-0.27.0 typing-extensions-4.7.1 uvicorn-0.23.2 websockets-11.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd3a09e-e5f0-4e27-82c2-c2e948155d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions==4.7.1 in /opt/conda/lib/python3.10/site-packages (4.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a899a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "import logging\n",
    "import nvidia_smi\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AutoPeftModelForCausalLM\n",
    ")\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from helpers.storj import Storj\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\",\n",
    "        metadata={\"help\": \"The model that you want to train from Huggingface. Defaults to Meta's Llama2 7B-chat and requires a HF login\"}\n",
    "    )\n",
    "    new_model_name: Optional[str] = field(\n",
    "        default=\"agora-llama-7b-chat\",\n",
    "        metadata={\"help\": \"The name for your fine-tuned model\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    hf_data_path: str = field(\n",
    "        default=\"iamtarun/python_code_instructions_18k_alpaca\",\n",
    "        metadata={\"help\": \"The path to the HF dataset. Defaults to `iamtarun/python_code_instructions_18k_alpaca`\"}\n",
    "    )\n",
    "    split: Optional[str] = field(\n",
    "        default=\"train\", #TODO: should this be default?,\n",
    "        metadata={\"help\": \"Which portion of the dataset you want to use\"}\n",
    "    )\n",
    "    personal_data: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path to your proprietary data\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingArguments(TrainingArguments):\n",
    "    # Specify an additional cache dir for files downloaded during training\n",
    "    # Usually things are downloaded into ~/.cache/huggingface\n",
    "    # Adding this is helpful for distributed training where all workers should read from a central cache\n",
    "    job_id : int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Unique id for the model training job\"}\n",
    "    )\n",
    "    bucket_name : Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the Storj bucket to upload/download checkpoints to and from\"}\n",
    "    )\n",
    "    cache_dir : Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    )\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Different models have different max lengths but this keeps it at a standard 512 incase you don't specify. Seq might be truncated\"}\n",
    "    )\n",
    "    output_dir : str = field(\n",
    "        default=\"./results\",\n",
    "        metadata={\"help\": \"Optional path where you want model checkpoints and final model to be saved\"}\n",
    "    ) \n",
    "    # num_train_epochs : int = field(\n",
    "    #     default=1,\n",
    "    #     metadata={\"help\": \"Number of training epochs\"}\n",
    "    # )\n",
    "    fp16 : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enable fp16 training\"}\n",
    "    )\n",
    "    bf16 : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable bf16 training. Only possible on A100 GPUs\"}\n",
    "    )\n",
    "    per_device_train_batch_size : int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Training batch size per device\"}\n",
    "    )\n",
    "    gradient_accumulation_steps : int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\"}\n",
    "    )\n",
    "    gradient_checkpointing : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \" If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"}\n",
    "    )\n",
    "    max_grad_norm : float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Maximum gradient normal (gradient clipping)\"}\n",
    "    )\n",
    "    learning_rate : float = field(\n",
    "        default=2e-4,\n",
    "        metadata={\"help\": \"Initial learning rate (AdamW optimizer)\"}\n",
    "    )\n",
    "    weight_decay : float = field(\n",
    "        default=0.001,\n",
    "        metadata={\"help\": \"Weight decay to apply to all layers except bias/LayerNorm weights\"}\n",
    "    )\n",
    "    optim : str = field(\n",
    "        default=\"paged_adamw_32bit\",\n",
    "        metadata={\"help\": \"Optimizer to use for training\"}\n",
    "    )\n",
    "    lr_scheduler_type : str = field(\n",
    "        default=\"constant\",\n",
    "        metadata={\"help\": \"Learning rate schedule (constant a bit better than cosine)\"}\n",
    "    )\n",
    "    warmup_ratio : float = field(\n",
    "        default=0.03,\n",
    "        metadata={\"help\": \"Ratio of steps for a linear warmup (from 0 to learning rate)\"}\n",
    "    )\n",
    "    group_by_length : bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Group sequences into batches with same length - Saves memory and speeds up training considerably\"}\n",
    "    )\n",
    "    save_steps : int = field(\n",
    "        default=10,\n",
    "        metadata={\"help\": \"Save checkpoint every X updates steps\"}\n",
    "    )\n",
    "    save_total_limit: int = field(\n",
    "        default=2,\n",
    "        metadata={}\n",
    "    )\n",
    "    logging_steps : int = field(\n",
    "        default=25,\n",
    "        metadata={\"help\": \"Log every X updates steps\"}\n",
    "    )\n",
    "    max_seq_length : int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\":\"Maximum sequence length to use\"}\n",
    "    )\n",
    "    packing : bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"Pack multiple short examples in the same input sequence to increase efficiency\"}\n",
    "    )\n",
    "    device_map : any = field(\n",
    "        default_factory=(lambda: {\"\":0}),\n",
    "        metadata={\"help\":\"Device mapping for the SFTTrainer\"}\n",
    "    )\n",
    "    max_steps: int = field(\n",
    "        default=20,\n",
    "        metadata={}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class QuanitzationArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    load_in_4bit: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Load a model in 4bit\"}\n",
    "    )\n",
    "    bnb_4bit_compute_dtype: torch.dtype = field(\n",
    "        default=torch.float16, \n",
    "        metadata={\"help\": \"Compute dtype for 4-bit base models\"}\n",
    "    )\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\", \n",
    "        metadata={\"help\": \"Quantization type (fp4 or nf4)\"}\n",
    "    )\n",
    "    use_nested_quant: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate nested quantization for 4-bit base models (double quantization)\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class QloraArguments():\n",
    "    # added all the params here in order to specify defaults\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=64, \n",
    "        metadata={\"help\": \"LoRA attention dimension\"}\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=16, \n",
    "        metadata={\"help\": \"Alpha parameter for LoRA scaling\"}\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.1, \n",
    "        metadata={\"help\": \"Dropout probability for LoRA layers\"}\n",
    "    )\n",
    "    bias: Optional[str] = field(\n",
    "        default=\"none\",\n",
    "        metadata={}\n",
    "    )\n",
    "    task_type: Optional[str] = field(\n",
    "        default=\"CAUSAL_LM\",\n",
    "        metadata={}\n",
    "    )\n",
    "\n",
    "class CheckpointCallback(TrainerCallback):\n",
    "    def __init__(self, training_args: TrainingArguments, storj: Storj) -> None:\n",
    "        self.training_args = training_args\n",
    "        self.storj = storj\n",
    "   \n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        self.storj.save_checkpoints_to_cloud(args.output_dir, state.global_step, args.job_id)\n",
    "        \n",
    "def huggingface_login():\n",
    "    # try: \n",
    "    #     HUGGING_FACE_TOKEN = os.environ['HUGGING_FACE_TOKEN']\n",
    "    # except KeyError:\n",
    "    #     raise Exception('Need to pass hugging face access token as environment variable.')\n",
    "\n",
    "    login(token=\"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\")\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    \"\"\"\n",
    "    - Get model state dict containing weights at time of call\n",
    "    - Convert to CPU tensors -> reduced memory?\n",
    "    - Delete original state dict to free VRAM\n",
    "    - _save() call to save it to disk/or external storage...?\n",
    "    \"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)\n",
    "\n",
    "def preprocess_data(source, tokenizer: PreTrainedTokenizer) -> Dict:\n",
    "    return {}\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "\n",
    "def build_bnb_config(quant_args) -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=quant_args.load_in_4bit,\n",
    "        bnb_4bit_quant_type=quant_args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=quant_args.bnb_4bit_compute_dtype\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def build_lora_config(qlora_args) -> LoraConfig:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=qlora_args.lora_alpha,\n",
    "        lora_dropout=qlora_args.lora_dropout,\n",
    "        r=qlora_args.lora_r,\n",
    "        bias=qlora_args.bias,\n",
    "        task_type=qlora_args.task_type\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "def finetune():\n",
    "    huggingface_login()\n",
    "\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, ModelTrainingArguments, QuanitzationArguments, QloraArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args, quant_args, qlora_args, remaining = parser.parse_args_into_dataclasses(return_remaining_strings=True) #TODO: remaining and the argument were added due to a weird error on Vast\n",
    "\n",
    "    # if bucket_name is not '', check for checkpoints in user's bucket\n",
    "    resume_from_checkpoint = False\n",
    "    if training_args.bucket_name:\n",
    "        storj = Storj(training_args.bucket_name)\n",
    "        resume_from_checkpoint = storj.pull_checkpoints_from_cloud(training_args)\n",
    "\n",
    "    bnb_config = build_bnb_config(quant_args=quant_args)\n",
    "    peft_config = build_lora_config(qlora_args=qlora_args)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset = load_dataset(data_args.hf_data_path, split=data_args.split)\n",
    "    dataset = dataset.remove_columns(['instruction', 'input', 'output']) #TODO: this is python dataset specific preprocessing. Will need to handle this inside preprocess function somehow\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  \n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\", #TODO: this will change based on dataset. I would add this as an optional default into DataArguments\n",
    "        max_seq_length=None,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "    if training_args.bucket_name:\n",
    "        trainer.add_callback(CheckpointCallback(training_args, storj))\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    trainer.save_state() #grabbed from skypilot but need to understand state better\n",
    "    print(\"state saved\")\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=\"results/finalsave\")\n",
    "    model.save_pretrained(\"results/usemodelsavep\")\n",
    "    print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91d79e41-69f3-4027-a634-1590b7a347a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302d47b4fc734278934f307b9f2ad859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state saved\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 313\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m safe_save_model_for_hf_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[1;32m    312\u001b[0m                                output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/finalsave\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 313\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/usemodelsavep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1817\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1813\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1814\u001b[0m     )\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1818\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1819\u001b[0m     )\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1824\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"
     ]
    }
   ],
   "source": [
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97fba5df-6904-4ba1-9594-807b35418a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lora_config() -> LoraConfig:\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "def build_bnb_config() -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "bnb = build_bnb_config()\n",
    "peft = build_lora_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4638259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, get_peft_model, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6a973da-6c75-42cb-b77e-4087206a8ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ed0ad6183d4bc685556bb9c7341227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"results/finalsave\", \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config = bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0da2fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80e8c9681e44fbb8196691a621907bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the following should be the same as above \n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        quantization_config = bnb,\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "peft_config_path = \"results/finalsave\"\n",
    "pmodel = PeftModel.from_pretrained(base_model, peft_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be6ac239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3533967360"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8929e1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3533967360"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pmodel)\n",
    "pmodel.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28f43192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2152f628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3737b",
   "metadata": {},
   "source": [
    "both ways to loading them in are equal! good thing to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "598baba3-e2e4-4ea8-bd30-fdbc75b04248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel \n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b72802-4b32-4c22-89d4-7eaad4caea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da44f6f2-d89b-4182-a7f6-a1c7c1fc6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3161d5d8-0870-49f9-9641-2332e0cdcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1475/3577709305.py:1: GradioDeprecationWarning: `server_name` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
      "  demo = gr.Interface.from_pipeline(llama_pipeline, server_name=\"0.0.0.0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://f6e212afee100e178c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f6e212afee100e178c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/routes.py\", line 507, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 641, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/pipelines.py\", line 222, in fn\n",
      "    data = pipeline(**data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 204, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1129, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1136, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1035, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 265, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1402, in generate\n",
      "    self._validate_model_class()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1197, in _validate_model_class\n",
      "    raise TypeError(exception_message)\n",
      "TypeError: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/routes.py\", line 507, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1109, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 641, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/pipelines.py\", line 222, in fn\n",
      "    data = pipeline(**data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 204, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1129, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1136, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1035, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 265, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1402, in generate\n",
      "    self._validate_model_class()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1197, in _validate_model_class\n",
      "    raise TypeError(exception_message)\n",
      "TypeError: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface.from_pipeline(llama_pipeline)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6a9cf-3e41-4344-aed2-45eafbf3ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to do a full save of the final model i think\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/llama2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
